# -*- coding: utf-8 -*-
"""laptop-price_prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i-hp2s8mFonz8ik8I5ugZ7w7cv8NZvQs

# Data Understanding

Dataset yang digunakan dalam proyek ini adalah dataset "Laptop Price" yang diperoleh dari Kaggle. Anda dapat mengunduh dataset tersebut dari tautan berikut: [Dataset Harga Laptop](https://www.kaggle.com/datasets/muhammetvarl/laptop-price/data).

## Variables in the Laptop Price Dataset:

- **Company:** Produsen laptop.
- **Product:** Merek dan model laptop.
- **TypeName:** Jenis laptop (Notebook, Ultrabook, Gaming, dll.).
- **Inches:** Ukuran layar laptop.
- **ScreenResolution:** Resolusi layar laptop.
- **Cpu:**  Central Processing Unit (CPU) laptop.
- **Ram:** Random Access Memory (RAM) laptop.
- **Memory:** Memori Hard Disk Drive (HDD) atau Solid State Drive (SSD) laptop.
- **Gpu:** Graphics Processing Unit (GPU) laptop.
- **OpSys:** Sistem Operasi laptop.
- **Weight:** Berat laptop.
- **Price_euros:** Harga laptop dalam Euro.

# Prepare Dataset

## Download Dataset
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

# Download dataset from kaggle
!kaggle datasets download -d muhammetvarl/laptop-price

!unzip '/content/laptop-price.zip' -d '/content'

"""## Import Library"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

"""## Load Dataset"""

data = pd.read_csv('/content/laptop_price.csv', sep=',', encoding='latin-1')
data.head()

"""# Data Wrangling

## Data Assessing
"""

data.info()

data.describe()

data.shape

"""## Transforming Data"""

# Function to extract features related to screen resolution
def extract_features_resolution(resolution):
    retina = 1 if 'Retina' in resolution else 0
    ips = 1 if 'IPS' in resolution else 0
    touchscreen = 1 if 'Touchscreen' in resolution else 0
    quad = 1 if 'Quad' in resolution else 0
    resolution_val = resolution.split()[-1] if len(resolution.split()) > 1 else resolution
    return retina, ips, touchscreen, quad, resolution_val

# Function to extract CPU features
def extract_cpu_features(cpu):
    brand = cpu.split(' ')[0]
    processor_type = ' '.join(cpu.split(' ')[1:3])
    clock_speed = cpu.split()[-1][:-3]
    return brand, processor_type, clock_speed

# Function to extract features related to memory
def extract_features_memory(memory):
    size = memory.split()[0]
    type_memory = memory.split()[1]
    extra_memory = 1 if '+' in memory else 0
    return size, type_memory, extra_memory

# Extract features and create new columns for each features
data['retina'], data['ips'], data['touchscreen'], data['quad'], data['resolution'] = zip(*data['ScreenResolution'].map(extract_features_resolution))
data['cpu_name'], data['cpu_type'], data['cpu_speed'] = zip(*data['Cpu'].map(extract_cpu_features))
data['memory_size'], data['type_memory'], data['extra_memory'] = zip(*data['Memory'].map(extract_features_memory))
data['gpu_brand'] = data['Gpu'].str.split().str[0]

# Convert memory size to integer (in GB) and handle TB
data['memory_size'] = data['memory_size'].apply(lambda x: int(float(x.replace('TB', '000').replace('GB', ''))))
data['memory_size'] = data['memory_size'].replace(1, 1000)

# Convert data type to integer and float
data['Ram'] = data['Ram'].str.replace('GB', '').astype(int)
data['Weight'] = data['Weight'].str.replace('kg', '').astype(float)
data['cpu_speed'] = data['cpu_speed'].astype(float)

# Split resolution into width and height columns and convert them to integers
data[['width', 'height']] = data['resolution'].str.split('x', expand=True).astype(int)

# Drop unused columns
data.drop(columns=['laptop_ID', 'ScreenResolution', 'Cpu', 'Memory', 'Gpu', 'resolution'], inplace=True)

# Rename columns to lowercase
column = {'Company': 'company',
         'Product': 'product',
         'TypeName': 'type_name',
         'Inches': 'inches',
         'Ram': 'ram',
         'OpSys': 'opsys',
         'Weight': 'weight',
         'Price_euros': 'price'}

data.rename(columns=column, inplace=True)

"""## Data Cleaning"""

data.isna().sum()

data.isnull().sum()

data.duplicated().sum()

# Remove duplicate rows
data.drop_duplicates(inplace=True)

data.info()

data.describe()

data.skew()

"""# EDA

## Univariate
"""

# Define numeric and categorical features
numeric_features = ['inches', 'ram', 'weight', 'retina', 'ips', 'touchscreen', 'quad', 'cpu_speed', 'memory_size', 'extra_memory', 'width', 'height']
categorical_features = ['company', 'type_name', 'opsys', 'cpu_name', 'cpu_type', 'type_memory', 'gpu_brand']

# Display distribution of categorial features
for feature in categorical_features:
    count = data[feature].value_counts()
    percent = 100*data[feature].value_counts(normalize=True)
    print(feature)
    df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
    print(df, '\n')

    plt.figure(figsize=(10, 6))
    count.plot(kind='bar', title=feature)
    plt.xlabel(feature)
    plt.ylabel('Jumlah Sampel')
    plt.xticks(rotation=45)
    plt.show()
    print('\n')

# Display histograms numeric features
data.hist(bins=50, figsize=(20,15))
plt.show()

"""## Multivariate"""

# Display distribution of categorial features on price labels
for col in categorical_features:
  sns.catplot(x=col, y="price", kind="bar", dodge=False, height = 4, aspect = 3,  data=data, palette="Set3")
  plt.title(f"Rata-rata 'price' Relatif terhadap - {col}")
  plt.xticks(rotation=45)
  plt.show()

# Display correlation numeric features
plt.figure(figsize=(10,8))
sns.heatmap(data.corr(numeric_only=True), annot = True, cmap = "YlGnBu")

sns.pairplot(data, y_vars=['price'], diag_kind='kde', height=3)
plt.show()

plt.figure(figsize = (10,8))
sns.heatmap(data.corr(numeric_only=True)[['price']].sort_values(by='price', ascending=False), annot = True, cmap = 'Reds')
plt.title('Target Correlation')
plt.show()

# Drop columns with weak correlation to price labels
data.drop(columns=['product', 'cpu_type', 'quad', 'retina', 'inches'], inplace=True, axis=1)
data.head()

"""# Data Preparation"""

# Select columns with data type 'object'
categorical_column = data.select_dtypes(include=['object']).columns.tolist()
categorical_column

# Encode categorical features into binary
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse=False)
encoded_data = encoder.fit_transform(data[categorical_column])
data = pd.concat([data, pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_column))], axis=1)
data.drop(columns=categorical_column, inplace=True, axis=1)

plt.figure(figsize = (10,20))
sns.heatmap(data.corr(numeric_only=True)[['price']].sort_values(by='price', ascending=False), annot = True, cmap = 'Reds')
plt.title('Target Correlation')
plt.show()

# Remove features with correlation less than 0.1
correlation = data.corrwith(data['price'])
correlation = correlation[(correlation < -0.1) | (correlation > 0.1)]
correlation = correlation.sort_values(ascending=False)

data = data.filter(items=correlation.index)

# Split to train and test dataset
from sklearn.model_selection import train_test_split

X = data.drop(columns=['price'])
y = data['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""# Modeling"""

# Create an empty DataFrame to store the mean squared error (MSE) for training and testing sets
models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['LinearRegression', 'RandomForest', 'GradienBoosting', 'NeuralNetwork', 'SVM'])

"""## Linear Regression"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Initialize the model
linear_reg = LinearRegression()

# Train the model
linear_reg.fit(X_train, y_train)

# Make predictions
models.loc['train_mse','LinearRegression'] = mean_squared_error(y_pred = linear_reg.predict(X_train), y_true=y_train)

"""## Random Forest"""

# Selecting the RandomForestRegressor as an algorithm
from sklearn.ensemble import RandomForestRegressor

# Initialize a model
rf = RandomForestRegressor()

# Train the model with the scaled train dataset
rf.fit(X_train, y_train)

models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred = rf.predict(X_train), y_true=y_train)

"""## Gradient Boosting"""

from sklearn.ensemble import GradientBoostingRegressor

# Initialize the model
gradient_boosting = GradientBoostingRegressor()

# Train the model
gradient_boosting.fit(X_train, y_train)

models.loc['train_mse','GradienBoosting'] = mean_squared_error(y_pred = gradient_boosting.predict(X_train), y_true=y_train)

"""## Neural Network"""

import tensorflow as tf

# Build the neural network model
neural_network = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1)
])

# Compile the model
neural_network.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
neural_network.fit(X_train, y_train, epochs=100, batch_size=32, verbose=2)

models.loc['train_mse', 'NeuralNetwork'] = mean_squared_error(y_train, neural_network.predict(X_train))

"""## SVM"""

from sklearn.svm import SVR

# Initialize the model
svm_regressor = SVR(kernel='linear')

# Train the model
svm_regressor.fit(X_train, y_train)

models.loc['train_mse','SVM'] = mean_squared_error(y_pred = svm_regressor.predict(X_train), y_true=y_train)

"""# Evaluate"""

# Create a dictionary mapping model names to their corresponding models
mse = pd.DataFrame(columns=['train', 'test'], index=['LinearRegression', 'RandomForest', 'GradienBoosting', 'NeuralNetwork', 'SVM'])
model_dict = {'LinearRegression': linear_reg, 'RandomForest': rf, 'GradienBoosting': gradient_boosting, 'NeuralNetwork': neural_network, 'SVM': svm_regressor}

# Calculate MSE for each model on both training and testing sets and store the results in the 'mse' DataFrame
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""# Testing"""

import random

random_index = random.randint(0, len(X_test) - 1)
prediksi = pd.DataFrame(X_test[random_index:random_index + 1].copy())
true_value = y_test[random_index:random_index + 1]
pred_dict = {'y_true': true_value}
for name, model in model_dict.items():
    prediction = model.predict(prediksi).round(1)
    pred_dict['prediksi_' + name] = prediction.flatten()  # Ensure the prediction is one-dimensional

pd.DataFrame(pred_dict)

